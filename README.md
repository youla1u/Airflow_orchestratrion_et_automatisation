# ðŸŒ¦ï¸Pipeline MÃ©tÃ©o avec Airflow (DuckDB + MinIO)

Ce projet est un **pipeline de donnÃ©es orchestrÃ© avec Airflow** qui illustre un cas concret de **data engineering**.  
Il ingÃ¨re des donnÃ©es mÃ©tÃ©o depuis une API publique, les stocke dans **DuckDB**, applique des transformations SQL, vÃ©rifie la qualitÃ©, puis exporte les rÃ©sultats vers **MinIO (S3)**.  

Lâ€™objectif est de montrer comment construire un **workflow ETL moderne** avec des technologies lÃ©gÃ¨res et open source.

---

## ðŸš€ FonctionnalitÃ©s principales

- **Ingestion** : rÃ©cupÃ©ration des donnÃ©es horaires de lâ€™API [Open-Meteo](https://open-meteo.com/).  
- **Stockage brut** : insertion dans DuckDB (`raw.weather_readings`) avec gestion dâ€™upsert via `MERGE`.  
- **Transformation** : agrÃ©gations journaliÃ¨res dans `mart.weather_daily`.  
- **QualitÃ© des donnÃ©es** : contrÃ´les de nulls, valeurs aberrantes et plages de validitÃ©.  
- **Export** : sauvegarde quotidienne au format CSV dans un bucket S3/MinIO via DuckDB `COPY ... TO`.  
- **Orchestration** : DAG Airflow avec dÃ©pendances, retries et gestion des erreurs.   
- **Notifications** : intÃ©gration Slack optionnelle.  

---

## ðŸ§± Architecture

```mermaid
flowchart TD
    A[Extract API Open-Meteo] --> B[DuckDB raw.weather_readings]
    B --> C[Transform -> mart.weather_daily]
    C --> D[Data Quality Checks]
    D --> E[Export CSV -> MinIO S3 Bucket]
    E --> F[Notification Slack/Logs]
